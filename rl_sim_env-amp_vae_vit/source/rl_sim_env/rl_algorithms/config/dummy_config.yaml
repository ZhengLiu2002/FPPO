algorithm:
  name: fppo
  # training parameters
  # -- advantage normalization
  normalize_advantage_per_mini_batch: false
  # -- value function
  value_loss_coef: 1.0
  cost_value_loss_coef: 1.0
  clip_param: 0.2
  use_clipped_value_loss: true
  # -- surrogate loss
  desired_kl: 0.01
  entropy_coef: 0.01
  gamma: 0.99
  lam: 0.95
  cost_gamma: null
  cost_lam: null
  cost_limit: 0.0
  delta_safe: 0.01
  backtrack_coeff: 0.5
  max_backtracks: 10
  projection_eps: 1.0e-8
  lagrange_lr: 0.01
  lagrange_max: 100.0
  focpo_eta: 0.02
  focpo_lambda: 1.0
  max_grad_norm: 1.0
  # -- training
  learning_rate: 0.001
  step_size: 0.001
  num_learning_epochs: 5
  num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches
  schedule: adaptive  # adaptive, fixed

policy:
  class_name: ActorCritic
  # for MLP i.e. `ActorCritic`
  activation: elu
  actor_hidden_dims: [128, 128, 128]
  critic_hidden_dims: [128, 128, 128]
  cost_critic_hidden_dims: null
  init_noise_std: 1.0
  noise_std_type: "scalar"  # 'scalar' or 'log'

  # only needed for `ActorCriticRecurrent`
  # rnn_type: 'lstm'
  # rnn_hidden_dim: 512
  # rnn_num_layers: 1

runner:
    num_steps_per_env: 24  # number of steps per environment per iteration
    max_iterations: 1500  # number of policy updates
    empirical_normalization: false
    # -- logging parameters
    save_interval: 50  # check for potential saves every `save_interval` iterations
    experiment_name: walking_experiment
    run_name: ""
    # -- logging writer
    logger: tensorboard  # tensorboard, neptune, wandb
    neptune_project: legged_gym
    wandb_project: legged_gym
    # -- load and resuming
    load_run: -1  # -1 means load latest run
    resume_path: null  # updated from load_run and checkpoint
    checkpoint: -1  # -1 means load latest checkpoint

runner_class_name: OnPolicyRunner
seed: 1
